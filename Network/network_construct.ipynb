{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3cd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Require library\n",
    "import ast\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# def top_n_dt_matrix(docs, n=100):\n",
    "#     # 문서를 문자열로 변환 (키워드를 밑줄로 연결)\n",
    "#     docs_as_strings = ['_'.join(doc) for doc in docs]\n",
    "    \n",
    "#     # CountVectorizer 초기화 (binary=False로 설정하여 빈도 수 계산)\n",
    "#     vectorizer = CountVectorizer(tokenizer=lambda x: x.split('_'), binary=False)\n",
    "    \n",
    "#     # DTM 생성\n",
    "#     dtm = vectorizer.fit_transform(docs_as_strings)\n",
    "    \n",
    "#     # 각 단어의 빈도 수 계산\n",
    "#     word_counts = np.sum(dtm, axis=0).A1  # A1은 np.array의 1차원 표현을 반환\n",
    "#     print(word_counts)\n",
    "    \n",
    "#     # 빈도 수별로 단어 정렬 (내림차순)\n",
    "#     top_indices = np.argsort(word_counts)[::-1][:n]  # 상위 n개 인덱스\n",
    "    \n",
    "#     # 상위 n개 단어에 해당하는 DTM 열 추출\n",
    "#     top_n_dtm = dtm[:, top_indices]\n",
    "    \n",
    "#     # 상위 n개의 특성 이름\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     top_n_feature_names = feature_names[top_indices]\n",
    "    \n",
    "#     return top_n_dtm.toarray(), top_n_feature_names\n",
    "\n",
    "def dt_matrix(docs, min_df=1):\n",
    "    # 문서를 문자열로 변환 (키워드를 밑줄로 연결)\n",
    "    docs_as_strings = ['_'.join(doc) for doc in docs]\n",
    "    \n",
    "    # CountVectorizer 초기화 (min_df 설정 추가)\n",
    "    vectorizer = CountVectorizer(binary=True, tokenizer=lambda x: x.split('_'), min_df=min_df)\n",
    "    \n",
    "    # DTM 생성\n",
    "    dtm = vectorizer.fit_transform(docs_as_strings).toarray()\n",
    "    \n",
    "    return dtm, vectorizer.get_feature_names_out()\n",
    "\n",
    "def co_matrix(matrix):\n",
    "    return np.dot(matrix.T, matrix)\n",
    "\n",
    "def create_graph_from_dtm(co_matrix, keywords, frequency):\n",
    "    # Co-occurence Matrix를 사용하여 그래프 생성\n",
    "    G = nx.from_numpy_array(co_matrix)\n",
    "    \n",
    "    # 각 노드에 빈도 값을 속성으로 설정\n",
    "    freq_dict = {i: freq for i, freq in enumerate(frequency)}\n",
    "    nx.set_node_attributes(G, freq_dict, \"frequency\")\n",
    "    \n",
    "    # 노드 레이블 설정\n",
    "    label_mapping = {i: keyword for i, keyword in enumerate(keywords)}\n",
    "    G = nx.relabel_nodes(G, label_mapping)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def normalize_by_diagonal(matrix):\n",
    "    # 대각선 값 추출\n",
    "    diagonal_values = np.diag(matrix).copy()\n",
    "    \n",
    "    # 0으로 나누는 것을 방지하기 위한 조건\n",
    "    diagonal_values[diagonal_values == 0] = 1\n",
    "    \n",
    "    # 각 행을 대각선 값으로 나누기\n",
    "    row_normalized = matrix / diagonal_values[:, None]\n",
    "    \n",
    "    # 각 열을 대각선 값으로 나누기\n",
    "    column_normalized = row_normalized / diagonal_values\n",
    "\n",
    "    return column_normalized, diagonal_values\n",
    "\n",
    "# def to_list(input_):\n",
    "#     if '[' in input_:\n",
    "#         return ast.literal_eval(input_)\n",
    "#     elif ',' in input_:\n",
    "#         return input_.split(', ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9507c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[innisfree hotels, senior vice president of de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[four seasons resort and residences whistler, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[auberge resorts collection, napa valley, hosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[executive vice president and general counsel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hri hospitality, new orleans, hotel, nashvill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>[uk employees feel undervalued at work, underv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>[international tourism, pre-pandemic levels, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>[u.s. households, international vacations, mmg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>[economic growth, unh, economic barometer, hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>[market recovery monitor, u.s. market recovery...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3252 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Keyphrase\n",
       "0     [innisfree hotels, senior vice president of de...\n",
       "1     [four seasons resort and residences whistler, ...\n",
       "2     [auberge resorts collection, napa valley, hosp...\n",
       "3     [executive vice president and general counsel,...\n",
       "4     [hri hospitality, new orleans, hotel, nashvill...\n",
       "...                                                 ...\n",
       "3247  [uk employees feel undervalued at work, underv...\n",
       "3248  [international tourism, pre-pandemic levels, a...\n",
       "3249  [u.s. households, international vacations, mmg...\n",
       "3250  [economic growth, unh, economic barometer, hou...\n",
       "3251  [market recovery monitor, u.s. market recovery...\n",
       "\n",
       "[3252 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"data/Hospitality_KT_Post\"\n",
    "data = pd.read_csv(file_name + \".csv\", index_col = 0)\n",
    "\n",
    "data = data[['Keyphrase']]\n",
    "data['Keyphrase'] = data['Keyphrase'].map(lambda x: ast.literal_eval(x))\n",
    "data.reset_index(drop=True, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a79344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 확인용\n",
    "# '''\n",
    "# print(len(set(sum(data['Keyphrase'].to_list(),[]))))\n",
    "\n",
    "# matrix, feature_names = dt_matrix(data['Keyphrase'])\n",
    "\n",
    "# coword_matrix = co_matrix(matrix)\n",
    "# print(len(coword_matrix))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cf26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cut_off = int(len(data) * 0.005)\n",
    "# matrix, feature_names = dt_matrix(data['Keyphrase'], cut_off)\n",
    "\n",
    "# coword_matrix = co_matrix(matrix)\n",
    "# coword_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658cc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "CPU times: total: 37.6 s\n",
      "Wall time: 37.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 21,   0,   1, ...,   0,   0,   0],\n",
       "       [  0,  36,   2, ...,   0,   0,   1],\n",
       "       [  1,   2, 116, ...,   0,   0,   4],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  35,  11,   1],\n",
       "       [  0,   0,   0, ...,  11,  16,   1],\n",
       "       [  0,   1,   4, ...,   1,   1,  49]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cut_off = int(len(data) * 0.005)\n",
    "print(cut_off)\n",
    "matrix, feature_names = dt_matrix(data['Keyphrase'], cut_off)\n",
    "\n",
    "coword_matrix = co_matrix(matrix)\n",
    "coword_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefbe6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix, feature_names = top_n_dt_matrix(data['Keyphrase'])\n",
    "\n",
    "# coword_matrix = co_matrix(matrix)\n",
    "# coword_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0879ef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.00041051, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.00047893, ..., 0.        , 0.        ,\n",
       "        0.00056689],\n",
       "       [0.00041051, 0.00047893, 0.        , ..., 0.        , 0.        ,\n",
       "        0.00070373],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.01964286,\n",
       "        0.00058309],\n",
       "       [0.        , 0.        , 0.        , ..., 0.01964286, 0.        ,\n",
       "        0.00127551],\n",
       "       [0.        , 0.00056689, 0.00070373, ..., 0.00058309, 0.00127551,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_matrix, diagonal_values = normalize_by_diagonal(coword_matrix.copy())\n",
    "np.fill_diagonal(normalized_matrix, 0) \n",
    "normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f422275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(array):\n",
    "    min_val = np.min(array)\n",
    "    max_val = np.max(array)\n",
    "    \n",
    "    # Min-Max scaling\n",
    "    scaled_array = (array - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return scaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d64c821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 993)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_array = min_max_scaling(normalized_matrix)\n",
    "scaled_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "32674c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'threshold': 0.0, 'remaining_nodes': 993, 'removed_nodes': 0},\n",
       " {'threshold': 0.1, 'remaining_nodes': 908, 'removed_nodes': 85},\n",
       " {'threshold': 0.2, 'remaining_nodes': 199, 'removed_nodes': 794},\n",
       " {'threshold': 0.30000000000000004,\n",
       "  'remaining_nodes': 47,\n",
       "  'removed_nodes': 946},\n",
       " {'threshold': 0.4, 'remaining_nodes': 31, 'removed_nodes': 962},\n",
       " {'threshold': 0.5, 'remaining_nodes': 12, 'removed_nodes': 981},\n",
       " {'threshold': 0.6000000000000001, 'remaining_nodes': 7, 'removed_nodes': 986},\n",
       " {'threshold': 0.7000000000000001, 'remaining_nodes': 6, 'removed_nodes': 987},\n",
       " {'threshold': 0.8, 'remaining_nodes': 6, 'removed_nodes': 987},\n",
       " {'threshold': 0.9, 'remaining_nodes': 4, 'removed_nodes': 989},\n",
       " {'threshold': 1.0, 'remaining_nodes': 0, 'removed_nodes': 993}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "n = len(feature_names)\n",
    "\n",
    "def remove_isolated_nodes(adj_matrix, features, diagonal_values, threshold):\n",
    "    filtered_matrix = np.where(adj_matrix > threshold, adj_matrix, 0)\n",
    "    isolated_nodes = np.where(~filtered_matrix.any(axis=1))[0]\n",
    "    if len(isolated_nodes) == 0:\n",
    "        return adj_matrix, features, diagonal_values\n",
    "    new_matrix = np.delete(filtered_matrix, isolated_nodes, axis=0)\n",
    "    new_matrix = np.delete(new_matrix, isolated_nodes, axis=1)\n",
    "    new_features = [feature for i, feature in enumerate(features) if i not in isolated_nodes]\n",
    "    new_diagonal_values = [value for i, value in enumerate(diagonal_values) if i not in isolated_nodes]\n",
    "    return new_matrix, new_features, new_diagonal_values\n",
    "\n",
    "def largest_subgraph(adj_matrix, features, diagonal_values):\n",
    "    if adj_matrix.size == 0:  # 인접 행렬이 비었을 경우, 빈 리스트 반환\n",
    "        return adj_matrix, features, diagonal_values\n",
    "\n",
    "    graph_csr = csr_matrix(adj_matrix)\n",
    "    n_components, labels = connected_components(csgraph=graph_csr, directed=False, return_labels=True)\n",
    "    \n",
    "    if n_components == 0:  # 연결된 컴포넌트가 없는 경우, 빈 리스트 반환\n",
    "        return np.array([]), []\n",
    "\n",
    "    component_sizes = np.bincount(labels)\n",
    "    largest_component_label = component_sizes.argmax()\n",
    "    nodes_in_largest_component = np.where(labels == largest_component_label)[0]\n",
    "    largest_subgraph_matrix = adj_matrix[nodes_in_largest_component][:, nodes_in_largest_component]\n",
    "    largest_subgraph_features = [features[i] for i in nodes_in_largest_component]\n",
    "    largest_diagonal_values = [diagonal_values[i] for i in nodes_in_largest_component]\n",
    "    return largest_subgraph_matrix, largest_subgraph_features, largest_diagonal_values\n",
    "\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.1)  # 0부터 1까지 0.1씩 증가\n",
    "largest_subgraphs_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    new_matrix, new_features, new_diagonal_values = remove_isolated_nodes(scaled_array, feature_names, diagonal_values, t)\n",
    "    largest_matrix, largest_features, largest_diagonal_values = largest_subgraph(new_matrix, new_features, new_diagonal_values)\n",
    "    largest_subgraphs_results.append({\n",
    "        'threshold': t,\n",
    "        'remaining_nodes': len(largest_features),\n",
    "        'removed_nodes': n - len(largest_features)\n",
    "    })\n",
    "largest_subgraphs_results  # 각 threshold 값에 따른 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a8e1a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix, new_features, new_diagonal_values = remove_isolated_nodes(scaled_array, feature_names, diagonal_values, 0.20358)\n",
    "largest_matrix, largest_features, largest_diagonal_values = largest_subgraph(new_matrix, new_features, new_diagonal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8ae662b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(largest_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2b928b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20412959543394327"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0이 아닌 요소들을 필터링합니다.\n",
    "non_zero_elements = largest_matrix[largest_matrix != 0]\n",
    "\n",
    "# 0이 아닌 요소들 중 최소값을 찾습니다.\n",
    "non_zero_min = np.min(non_zero_elements)\n",
    "non_zero_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dd63d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d24361c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph_from_dtm(largest_matrix, largest_features, largest_diagonal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc4ae4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = file_name+'_'+str(1005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a9ddd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, save_path+\".gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "data = nx.node_link_data(G)\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "# with open(save_path+'.json', 'w') as outfile:\n",
    "#     json.dump(data, outfile, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9544285",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add the positions to the nodes in the data\n",
    "for node in data['nodes']:\n",
    "    node_id = node['id']\n",
    "    if node_id in pos:\n",
    "        node['position'] = pos[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b88f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path+'.json', 'w') as outfile:\n",
    "    json.dump(data, outfile, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4a822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
